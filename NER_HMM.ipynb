{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sXm2wt519o"
   },
   "source": [
    "# HMM for NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEyk2ch1GYi0"
   },
   "source": [
    "### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_hx0PKdII9C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmssMqR2IKkC"
   },
   "source": [
    "Write a function to load sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4MKd94oHDKr"
   },
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace all digits in a string with zeros.\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\d\", \"0\", s)\n",
    "\n",
    "\n",
    "def load_sentences(path):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, \"r\", \"utf8\"):\n",
    "        line = zero_digits(line.rstrip())\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if \"DOCSTART\" not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if \"DOCSTART\" not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W76Clg4J2wo"
   },
   "source": [
    "Prepare the training/test data using the loaded sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yS9JsVSMZ6l"
   },
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_7t9tVCMdFY"
   },
   "outputs": [],
   "source": [
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "\n",
    "def word_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico[\"<UNK>\"] = 10000000\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\n",
    "        \"Found %i unique words (%i in total)\" % (len(dico), sum(len(x) for x in words))\n",
    "    )\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNS57L87IQdT"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
    "        - words (strings)\n",
    "        - word indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    assert mode == \"train\" or (mode == \"test\" and word_to_id and tag_to_id)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
    "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
    "\n",
    "    def f(x):\n",
    "        return x\n",
    "\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[f(w) if f(w) in word_to_id else \"<UNK>\"] for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append(\n",
    "            {\n",
    "                \"str_words\": str_words,\n",
    "                \"words\": words,\n",
    "                \"tags\": tags,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return data, {\n",
    "            \"word_to_id\": word_to_id,\n",
    "            \"id_to_word\": id_to_word,\n",
    "            \"tag_to_id\": tag_to_id,\n",
    "            \"id_to_tag\": id_to_tag,\n",
    "        }\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgikvX-qKTL7"
   },
   "source": [
    "### Hidden Markov Model\n",
    "\n",
    "In this section, we will implement a bigram hidden markov model (HMM) that could perform two types of decoding: greedy decoding and viterbi decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "te89fFIgLdFf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCiZJCYKMln5"
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"\n",
    "    HMM Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic, decode_type):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_words = len(dic[\"word_to_id\"])\n",
    "        self.num_tags = len(dic[\"tag_to_id\"])\n",
    "\n",
    "        self.initial_prob = np.ones([self.num_tags])\n",
    "        self.transition_prob = np.ones([self.num_tags, self.num_tags])\n",
    "        self.emission_prob = np.ones([self.num_tags, self.num_words])\n",
    "        self.decode_type = decode_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        TODO: Train a bigram HMM model using MLE estimates.\n",
    "        Complete the code to compute self,initial_prob, self.transition_prob and self.emission_prob appropriately.\n",
    "\n",
    "        Args:\n",
    "            corpus is a list of dictionaries of the form:\n",
    "            {'str_words': str_words,   ### List of string words\n",
    "            'words': words,            ### List of word IDs\n",
    "            'tags': tags}              ### List of tag IDs\n",
    "            All three lists above have length equal to the sentence length for each instance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute initial_probs.\n",
    "        # initial_prob[x]: the probability of tag x to be assigned to the first token in a sentence.\n",
    "        self.initial_prob = np.zeros([self.num_tags])\n",
    "        for sentence in corpus:\n",
    "            # TODO: update self.initial_prob\n",
    "            self.initial_prob[sentence[\"tags\"][0]] += 1\n",
    "\n",
    "        # Normarlize initial_prob to sum to 1\n",
    "        self.initial_prob /= np.sum(self.initial_prob)\n",
    "\n",
    "        # Step 2: Complete the code to compute transition_prob.\n",
    "        # transition_prob[x][y]: the probability of tag y to appear after tag x.\n",
    "        self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
    "        for sentence in corpus:\n",
    "            tag = sentence[\"tags\"]\n",
    "            for i in range(1, len(tag)):\n",
    "                # TODO: update self.transition_prob\n",
    "                self.transition_prob[tag[i - 1], tag[i]] += 1\n",
    "\n",
    "        # Normalize every row of transition_prob to sum to 1.\n",
    "        for p in self.transition_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "        # Step 3: Complete the code to compute emission_prob\n",
    "        # emission_prob[x][y]: the probability of word y to appear given tag x.\n",
    "        # Note that for each sentence s in the corpus, word IDs are in s['words'].\n",
    "        self.emission_prob = np.zeros([self.num_tags, self.num_words])\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence[\"tags\"])):\n",
    "                # TODO: update self.emission_prob\n",
    "                self.emission_prob[sentence[\"tags\"][i], sentence[\"words\"][i]] += 1\n",
    "\n",
    "        # For every tag, normalize the emission_prob to sum to 1.\n",
    "        for p in self.emission_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "        return\n",
    "\n",
    "    def greedy_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        Decode a single sentence in Greedy fashion\n",
    "        Return a list of tags.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "\n",
    "        init_scores = [\n",
    "            self.initial_prob[t] * self.emission_prob[t][sentence[0]]\n",
    "            for t in range(self.num_tags)\n",
    "        ]\n",
    "        tags.append(np.argmax(init_scores))\n",
    "\n",
    "        for w in sentence[1:]:\n",
    "            scores = [\n",
    "                self.transition_prob[tags[-1]][t] * self.emission_prob[t][w]\n",
    "                for t in range(self.num_tags)\n",
    "            ]\n",
    "            tags.append(np.argmax(scores))\n",
    "\n",
    "        assert len(tags) == len(sentence)\n",
    "        return tags\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        TODO: Complete the code to decode a single sentence using the Viterbi algorithm.\n",
    "        Args:\n",
    "             sentence -- a list of ints that represents word IDs.\n",
    "        Output:\n",
    "             tags     -- a list of ints that represents the tags decoded from the input.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "        l = len(sentence)\n",
    "\n",
    "        pi = self.initial_prob\n",
    "        A = self.transition_prob\n",
    "        O = self.emission_prob\n",
    "\n",
    "        # Let M be the state matrix.\n",
    "        # M[i,j]: most probable sequence of tags ending with tag j at the i-th token.\n",
    "        M = np.zeros((l, self.num_tags))\n",
    "        M[:, :] = float(\"-inf\")\n",
    "\n",
    "        # Use B to track the path to reach the most probable sequence.\n",
    "        # B[i,j] is the tag of the (i-1)-th token in the most probable sequence ending with tag j at the i-th token.\n",
    "        B = np.zeros((l, self.num_tags), \"int\")\n",
    "\n",
    "        # Compute the probability to assign each tag to the first token.\n",
    "        M[0, :] = pi * O[:, sentence[0]]\n",
    "\n",
    "        # Dynamic programming.\n",
    "        for i in range(1, l):\n",
    "            for j in range(self.num_tags):\n",
    "                # TODO: Compute M[i, j] and B[i, j].\n",
    "                pass\n",
    "\n",
    "        # Extract the optimal sequence of tags from B.\n",
    "        # Start from the last position, and iteratively find the tag of each position that results in the most probable tag sequence.\n",
    "        tags.append(np.argmax(M[l - 1, :]))\n",
    "        for i in range(l - 1, 0, -1):\n",
    "            # TODO: Extract the tag of the (i-1)-th token that results in the most probable sequence of tags.\n",
    "            pass\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"\n",
    "        Tag a sentence using a trained HMM.\n",
    "        \"\"\"\n",
    "        if self.decode_type == \"viterbi\":\n",
    "            return self.viterbi_decode(sentence)\n",
    "        elif self.decode_type == \"greedy\":\n",
    "            return self.greedy_decode(sentence)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown decoding type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6P0Qs5JM0x3"
   },
   "source": [
    "### Train and evaluate HMMs.\n",
    "\n",
    "This section contains driver code for learning a HMM for named entity recognition on a training corpus and evaluating it on a test corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJRRLA2jOsl1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk2KF59LOzSr"
   },
   "source": [
    "Write a function to tag the test corpus with a trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldGSAHWSOwt1"
   },
   "outputs": [],
   "source": [
    "def tag_corpus(model, test_corpus, output_file, dic):\n",
    "    if output_file:\n",
    "        f_output = codecs.open(output_file, \"w\", \"utf-8\")\n",
    "    start = time.time()\n",
    "\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    y_pred = []\n",
    "    y_actual = []\n",
    "    print(\"Tagging...\")\n",
    "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
    "        tags = model.tag(sentence[\"words\"])\n",
    "        str_tags = [dic[\"id_to_tag\"][t] for t in tags]\n",
    "        y_pred.extend(tags)\n",
    "        y_actual.extend(sentence[\"tags\"])\n",
    "\n",
    "        # Check accuracy.\n",
    "        num_correct += np.sum(np.array(tags) == np.array(sentence[\"tags\"]))\n",
    "        num_total += len([w for w in sentence[\"words\"]])\n",
    "\n",
    "        if output_file:\n",
    "            f_output.write(\n",
    "                \"%s\\n\"\n",
    "                % \" \".join(\n",
    "                    \"%s%s%s\" % (w, \"__\", y)\n",
    "                    for w, y in zip(sentence[\"str_words\"], str_tags)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        \"---- %i lines tagged in %.4fs ----\" % (len(test_corpus), time.time() - start)\n",
    "    )\n",
    "    if output_file:\n",
    "        f_output.close()\n",
    "\n",
    "    print(\"Overall accuracy: %s\\n\" % (num_correct / num_total))\n",
    "    return y_pred, y_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhAjb7OuPEgT"
   },
   "source": [
    "Write a function a compute the confusion matrix and the F-1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC_VhUG0O98M"
   },
   "outputs": [],
   "source": [
    "def compute_score(y_pred, y_actual):\n",
    "    A = confusion_matrix(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred, average=None)\n",
    "    print(\"Confusion Matrix:\\n\", A)\n",
    "    print(\"F-1 scores: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLFBiD4BPMl1"
   },
   "source": [
    "Write a function to train and evalute the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFcYYThhPRbh"
   },
   "outputs": [],
   "source": [
    "def runHiddenMarkovModel(train_corpus, test_corpus, dic, decode_type, output_file):\n",
    "    # build and train the model\n",
    "    model = HMM(dic, decode_type)\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    print(\"Train results:\")\n",
    "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic[\"id_to_tag\"])\n",
    "    A = compute_score(pred, real)\n",
    "\n",
    "    # test on validation\n",
    "    print(\"\\n-----------\\nTest results:\")\n",
    "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic[\"id_to_tag\"])\n",
    "    A = compute_score(pred, real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vRsc-YrRAb2"
   },
   "source": [
    "### Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McZ9KpeQRlA9"
   },
   "source": [
    "#### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjVwUkR9Rq53"
   },
   "outputs": [],
   "source": [
    "train_file = \"eng.train\"\n",
    "test_file = \"eng.val\"\n",
    "\n",
    "# Load the training data\n",
    "train_sentences = load_sentences(train_file)\n",
    "train_corpus, dic = prepare_dataset(\n",
    "    train_sentences, mode=\"train\", word_to_id=None, tag_to_id=None\n",
    ")\n",
    "\n",
    "# Load the testing data\n",
    "test_sentences = load_sentences(test_file)\n",
    "test_corpus = prepare_dataset(\n",
    "    test_sentences,\n",
    "    mode=\"test\",\n",
    "    word_to_id=dic[\"word_to_id\"],\n",
    "    tag_to_id=dic[\"tag_to_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzNnUiBZS4ME"
   },
   "source": [
    "#### Experiment with an HMM with greedy decoding for Problem 2(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0t7W9JMTfLl"
   },
   "outputs": [],
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus=train_corpus,\n",
    "    test_corpus=test_corpus,\n",
    "    dic=dic,\n",
    "    decode_type=\"greedy\",\n",
    "    output_file=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP4iMShYX3Xp"
   },
   "source": [
    "#### Experiment with an HMM with viterbi decoding for Problem 2(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zBEh-TpX8KC"
   },
   "outputs": [],
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus=train_corpus,\n",
    "    test_corpus=test_corpus,\n",
    "    dic=dic,\n",
    "    decode_type=\"viterbi\",\n",
    "    output_file=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
