{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6YTnpgbFdMI",
    "outputId": "e8a79660-2c2b-4a57-cf57-1cf069eb7c56"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.2.0 torchtext==0.17.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install -U scikit-learn\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c62StNb2NvKk",
    "outputId": "03be5bda-0ee1-40d8-bdd2-38b3b9f6eb81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.2.0+rocm5.7\n",
      "Torchtext Version: 0.17.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Torchtext Version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EjRM4cCFRh-d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn1bIPjAN-9V"
   },
   "source": [
    "## Feedforward Neural Network (FFNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJOKIneRTrTH"
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVt1a6nzWsiF"
   },
   "source": [
    "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooQEJEafWCcd"
   },
   "source": [
    "First, we write a dataloader for loading the dataset into mini-batches used for training the model. See [torch.utils.data](https://pytorch.org/docs/stable/data.html) for how dataloaders work in PyTorch. In short, we typically need to do two things:\n",
    "\n",
    "1. Define a [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets) by subclassing [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and overriding 3 methods: `__init__`, `__getitem__`, and `__len__`.\n",
    "1. Create a [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) by calling its constructor. We have to specify the dataset and a few hyperparameters such as batch size.\n",
    "\n",
    "Most of the work has been done by us. As a simple exercise, try to understand the code and implement `__len__`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WnNfOBUYJvVW"
   },
   "outputs": [],
   "source": [
    "# A sentence is a list of (word, tag) tuples.\n",
    "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
    "Sentence = List[Tuple[str, str]]\n",
    "\n",
    "\n",
    "def read_data_file(\n",
    "    datapath: str,\n",
    ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Read and preprocess input data from the file `datapath`.\n",
    "    Example:\n",
    "    ```\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
    "    ```\n",
    "    Return values:\n",
    "        `sentences`: a list of sentences, including words and NER tags\n",
    "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
    "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
    "    \"\"\"\n",
    "    sentences: List[Sentence] = []\n",
    "    word_cnt: Dict[str, int] = Counter()\n",
    "    tag_cnt: Dict[str, int] = Counter()\n",
    "\n",
    "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
    "        if \"DOCSTART\" in sentence_txt:\n",
    "            # Ignore dummy sentences at the begining of each document.\n",
    "            continue\n",
    "        # Read a new sentence\n",
    "        sentences.append([])\n",
    "        for token in sentence_txt.split(\"\\n\"):\n",
    "            w, _, _, t = token.split()\n",
    "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
    "            w = re.sub(\"\\d\", \"0\", w)\n",
    "            word_cnt[w] += 1\n",
    "            tag_cnt[t] += 1\n",
    "            sentences[-1].append((w, t))\n",
    "\n",
    "    return sentences, word_cnt, tag_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFVnIjA8KEe0"
   },
   "source": [
    "## Implement the `__len__` function below **(1 point)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9RGv1K0pP1bR"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each data example is a word, its NER tag (the target), and a fixed window centered around it (the input).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        datapath: str,\n",
    "        window_size: int,\n",
    "        words_vocab: Optional[Vocab] = None,\n",
    "        tags_vocab: Optional[Vocab] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset by reading from datapath.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "        START = \"<START>\"\n",
    "        END = \"<END>\"\n",
    "        UNKNOWN = \"<UNKNOWN>\"\n",
    "\n",
    "        print(\"Loading data from %s\" % datapath)\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
    "\n",
    "        # Extract windows\n",
    "        for sent in sentences:\n",
    "            words = [START for _ in range(window_size)]\n",
    "            tags = [None for _ in range(window_size)]\n",
    "            for w, t in sent:\n",
    "                words.append(w)\n",
    "                tags.append(t)\n",
    "            words.extend([END for _ in range(window_size)])\n",
    "            tags.extend([None for _ in range(window_size)])\n",
    "\n",
    "            for i, t in enumerate(tags[window_size:-window_size], start=window_size):\n",
    "                self.examples.append(\n",
    "                    {\n",
    "                        \"word\": words[i],\n",
    "                        \"tag\": t,\n",
    "                        \"context\": words[i - window_size : i + window_size + 1],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print(\"%d examples loaded.\" % len(self.examples))\n",
    "\n",
    "        # set vocabs\n",
    "        if words_vocab is None:\n",
    "            words_vocab = vocab(word_cnt, specials=[START, END, UNKNOWN]) # automatically create a vocabulary from words in dataset\n",
    "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
    "        self.words_vocab = words_vocab\n",
    "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
    "        self.start_idx = self.words_vocab[START]\n",
    "        self.end_idx = self.words_vocab[END]\n",
    "\n",
    "        if tags_vocab is None:\n",
    "            tags_vocab = vocab(tag_cnt, specials=[]) # automatically create tags vocabulary from tags in dataset\n",
    "        self.tags_vocab = tags_vocab\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the idx'th example in the dataset.\n",
    "        Convert words and the tag to indexes.\n",
    "        \"\"\"\n",
    "        example = self.examples[idx]\n",
    "        word = example[\"word\"]\n",
    "        tag = example[\"tag\"]\n",
    "        context = example[\"context\"]\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"word_idx\": self.words_vocab[word],\n",
    "            \"tag\": tag,\n",
    "            \"tag_idx\": self.tags_vocab[tag],\n",
    "            \"context\": context,\n",
    "            \"context_idxs\": torch.tensor(\n",
    "                [self.words_vocab[w] for w in context]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of examples in the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        return len(self.examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "F-aHcN5yJyoa"
   },
   "outputs": [],
   "source": [
    "def create_fixed_window_dataloaders(\n",
    "    batch_size: int, window_size: int, shuffle: bool = True\n",
    ") -> Tuple[DataLoader, DataLoader, Dict[str, Vocab]]:\n",
    "    \"\"\"\n",
    "    Create the dataloaders for training and validaiton.\n",
    "    \"\"\"\n",
    "    ds_train = FixedWindowDataset(\"eng.train\", window_size)\n",
    "    # Re-use the vocabulary of the training data\n",
    "    ds_val = FixedWindowDataset(\"eng.val\", window_size, words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
    "    loader_train = DataLoader(\n",
    "        ds_train, batch_size, shuffle, drop_last=True, pin_memory=True\n",
    "    )\n",
    "    loader_val = DataLoader(ds_val, batch_size, pin_memory=True)\n",
    "    return loader_train, loader_val, ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODfPyQjPSmCv"
   },
   "source": [
    "Let's test our dataloader. Try to understand the output, as it will save your time later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "Zmt9c9svgzy8",
    "outputId": "cce6bfd1-8029-4220-84be-0c855807c25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "Iterating on the training data..\n",
      "{'word': ['EU', 'rejects', 'German'], 'word_idx': tensor([3, 4, 5]), 'tag': ['ORG', 'O', 'MISC'], 'tag_idx': tensor([0, 1, 2]), 'context': [('<START>', '<START>', 'EU'), ('<START>', 'EU', 'rejects'), ('EU', 'rejects', 'German'), ('rejects', 'German', 'call'), ('German', 'call', 'to')], 'context_idxs': tensor([[0, 0, 3, 4, 5],\n",
      "        [0, 3, 4, 5, 6],\n",
      "        [3, 4, 5, 6, 7]])}\n",
      "5\n",
      "torch.Size([3, 5])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def check_fixed_window_dataloader() -> None:\n",
    "    loader_train, _, _ = create_fixed_window_dataloaders(\n",
    "        batch_size=3, window_size=2, shuffle=False\n",
    "    )\n",
    "    print(\"Iterating on the training data..\")\n",
    "    for i, data_batch in enumerate(loader_train):\n",
    "        if i == 0:\n",
    "            print(data_batch)\n",
    "            print(len(data_batch[\"context\"]))\n",
    "            print(data_batch[\"context_idxs\"].shape)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "check_fixed_window_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR_aJ-FincuN"
   },
   "source": [
    "### Implement the Model **(4 points)**\n",
    "\n",
    "Next, let's implement feedforward neural networks following the description of Problem 1 in Assignment #3.\n",
    "\n",
    "Models in PyTorch are subclasses of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You have to override `__init__` for initializing the model and `forward` for calculating the forward pass. Checkout this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html#) if you are not sure how torch.nn.Module works.\n",
    "\n",
    "PyTorch provides a wide array of [neural network layers](https://pytorch.org/docs/stable/nn.html) as building blocks for your model. Here are some of them that may be relevant:\n",
    "\n",
    "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid) or [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
    "\n",
    "Note a difference with Problem 3 of Assignment #2 is that we do not apply softmax when calculatinng $\\hat{y}^{(t)}$. Instead, we leave what softmax does to the loss function [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy). For details, please see its difference with [F.nll_loss](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Hx0oMgVffSD1"
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward Neural Networks for NER\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, words_vocab: Vocab, tags_vocab: Vocab, window_size: int, d_emb: int, d_hidden: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a two-layer feedforward neural network with sigmoid activation.\n",
    "        Parameters:\n",
    "            `words_vocab`: vocabulary of words\n",
    "            `tags_vocab`: vocabulary of tags\n",
    "            `window_size`: size of the context window (w in Problem 3 of Assignment #2)\n",
    "            `d_emb`: dimension of word embeddings (D in Problem 3 of Assignment #2)\n",
    "            `d_hidden`: dimension of the hidden layer (H in Problem 3 of Assignment #2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Create the word embeddings (nn.Embedding),\n",
    "        self.words_vocab = words_vocab\n",
    "        self.tags_vocab = tags_vocab\n",
    "        self.window_size = window_size\n",
    "        self.d_emb = d_emb\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
    "        self.hidden_layer = nn.Linear((2 * window_size + 1) * d_emb, d_hidden)\n",
    "        self.output_layer = nn.Linear(d_hidden, len(tags_vocab))\n",
    "\n",
    "    def forward(self, context_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given the word indexes in a context window, predict the logits of the NER tag.\n",
    "        Parameters:\n",
    "            `context_idxs`: a batch_size x (2 * window_size + 1) tensor\n",
    "                          context_idxs[i] contains word indexes in the window of the i'th data example.\n",
    "        Return values:\n",
    "            `logits`: a batch_size x 5 tensor (\\hat{y}^{(t)} in Problem 3 of Assignment #2, without softmax)\n",
    "                    logits[i][j] is the output score (before softmax) of the i'th example for tag j.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the two-layer FFNN with sigmoid hidden layer.\n",
    "        #       Do not apply softmax, since we will use F.cross_entropy as the loss function.\n",
    "        \n",
    "        context_emb = self.embedding(context_idxs)\n",
    "        batch_size, _, _ = context_emb.size()\n",
    "        context_flat = context_emb.view(batch_size, -1)\n",
    "        hidden_output = torch.sigmoid(self.hidden_layer(context_flat))\n",
    "        logits = self.output_layer(hidden_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyZCvfOMR7YP"
   },
   "source": [
    "Optionally, let's do a simple sanity check of your implementation. In `check_ffnn`, we load a batch of data examples and pass it through the FFNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WLMGYSZ7KxzP"
   },
   "outputs": [],
   "source": [
    "# Some helper code\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Use GPU when it is available; use CPU otherwise.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "H7jikP_1fZP-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=448, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "Input tensor shape: torch.Size([3, 7])\n",
      "Output tensor shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "def check_ffnn() -> None:\n",
    "  # Hyperparameters\n",
    "  batch_size = 3\n",
    "  d_emb = 64\n",
    "  d_hidden = 128\n",
    "  window_size = 3\n",
    "  # Create the dataloaders and the model\n",
    "  loader_train, _, ds_train = create_fixed_window_dataloaders(batch_size, window_size)\n",
    "  model = FFNN(ds_train.words_vocab, ds_train.tags_vocab, window_size, d_emb, d_hidden)\n",
    "  device = get_device()\n",
    "  model.to(device)\n",
    "  print(model)\n",
    "  # Get the first batch\n",
    "  data_batch = next(iter(loader_train))\n",
    "  # Move data to GPU\n",
    "  context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "  tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "  # Calculate the model\n",
    "  print(\"Input tensor shape:\", context_idxs.size())\n",
    "  logits = model(context_idxs)\n",
    "  print(\"Output tensor shape:\", logits.size())\n",
    "\n",
    "check_ffnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PkRIOgeXIng0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n"
     ]
    }
   ],
   "source": [
    "ds_train = FixedWindowDataset(\"eng.train\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0angAnEno9v"
   },
   "source": [
    "### Training and Validation **(4 points)**\n",
    "\n",
    "Having implemented the model, the next step is to implement functions for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vWcKwiIMjekY"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate various evaluation metrics such as accuracy and F1 score\n",
    "    Parameters:\n",
    "        `ground_truth`: the list of ground truth NER tags\n",
    "        `predictions`: the list of predicted NER tags\n",
    "    \"\"\"\n",
    "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
    "        \"average f1\": np.mean(f1_scores),\n",
    "        \"f1\": f1_scores,\n",
    "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_ffnn(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    silent: bool = False,  # whether to print the training loss\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train the FFNN model.\n",
    "    Return values:\n",
    "        1. the average training loss\n",
    "        2. training metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    report_interval = 100\n",
    "\n",
    "    for i, data_batch in enumerate(loader):\n",
    "        context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "        tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
    "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
    "        # 3. Perform the backward pass to calculate the gradient.\n",
    "        # 4. Use the optimizer to update model parameters.\n",
    "        # Caveat: You may need to call optimizer.zero_grad(). Figure out what it does!\n",
    "        \n",
    "        logits = model(context_idxs)\n",
    "        loss = F.cross_entropy(logits, tag_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ground_truth.extend(tag_idx.tolist())\n",
    "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "        if not silent and i > 0 and i % report_interval == 0:\n",
    "            print(\n",
    "                \"\\t[%06d/%06d] Loss: %f\"\n",
    "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
    "            )\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def validate_ffnn(\n",
    "    model: nn.Module, loader: DataLoader, device: torch.device\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate the FFNN model.\n",
    "    Return values:\n",
    "        1. the average validation loss\n",
    "        2. validation metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data_batch in loader:\n",
    "            context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "            tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "            # TODO: Similar to what you did in train_ffnn, but only step 1 and 2.\n",
    "\n",
    "            logits = model(context_idxs)\n",
    "            loss = F.cross_entropy(logits, tag_idx)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            ground_truth.extend(tag_idx.tolist())\n",
    "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def train_val_loop_ffnn(hyperparams: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Train and validate the FFNN model for a number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Hyperparameters:\", hyperparams)\n",
    "    # Create the dataloaders\n",
    "    loader_train, loader_val, ds_train = create_fixed_window_dataloaders(\n",
    "        hyperparams[\"batch_size\"], hyperparams[\"window_size\"]\n",
    "    )\n",
    "    # Create the model\n",
    "    model = FFNN(\n",
    "        ds_train.words_vocab,\n",
    "        ds_train.tags_vocab,\n",
    "        hyperparams[\"window_size\"],\n",
    "        hyperparams[\"d_emb\"],\n",
    "        hyperparams[\"d_hidden\"],\n",
    "    )\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # Create the optimizer\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
    "    )\n",
    "\n",
    "    # Train and validate\n",
    "    for i in range(hyperparams[\"num_epochs\"]):\n",
    "        print(\"*\" * 80 )\n",
    "        print(f\"Epoch #{i+1}\")\n",
    "\n",
    "        print(\"Training..\")\n",
    "        loss_train, metrics_train = train_ffnn(\n",
    "            model, loader_train, optimizer, device, silent=True\n",
    "        )\n",
    "        print(\"Training loss: \", loss_train)\n",
    "        print(\"Training metrics:\")\n",
    "        for k, v in metrics_train.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "        print(\"Validating..\")\n",
    "        loss_val, metrics_val = validate_ffnn(model, loader_val, device)\n",
    "        print(\"Validation loss: \", loss_val)\n",
    "        print(\"Validation metrics:\")\n",
    "        for k, v in metrics_val.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "    print(\"************ Training Done! ************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbuC7sYHX3Wl"
   },
   "source": [
    "We are ready to run experiments! Let's train the model for 5 epochs, with `window_size=2`. After each epoch, we perform validation and print the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5ZTOFj9NXl65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 2, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=320, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.1855672836322478\n",
      "Training metrics:\n",
      "\t accuracy :  0.9452485437657431\n",
      "\t average f1 :  0.8068615330506621\n",
      "\t f1 :  [0.75057813 0.97602672 0.69605096 0.83370184 0.77795002]\n",
      "\t confusion matrix :  [[  6816   2066    205    407    515]\n",
      " [   480 167453    152    527    666]\n",
      " [   249   1239   2732    146    216]\n",
      " [   265   1758     54   8861    172]\n",
      " [   343   1338    125    206   6273]]\n",
      "Validating..\n",
      "Validation loss:  0.13264973281669276\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9633296662999633\n",
      "\t average f1 :  0.8652779688559615\n",
      "\t f1 :  [0.80175353 0.98444324 0.78136986 0.8843355  0.8744877 ]\n",
      "\t confusion matrix :  [[ 1646   365    39    85   115]\n",
      " [   63 40911    46    86    58]\n",
      " [   39   209   713    19    27]\n",
      " [   39   317     3  2309    22]\n",
      " [   69   149    17    33  1707]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.03525614524870097\n",
      "Training metrics:\n",
      "\t accuracy :  0.990126141372796\n",
      "\t average f1 :  0.9611107235395444\n",
      "\t f1 :  [0.94257386 0.99675373 0.92720817 0.98118981 0.95782805]\n",
      "\t confusion matrix :  [[  9364    297     93     63    187]\n",
      " [   181 168875     80     78     69]\n",
      " [   104    209   4178     34     61]\n",
      " [    71     77     21  10902     34]\n",
      " [   145    109     54     40   7938]]\n",
      "Validating..\n",
      "Validation loss:  0.14677211017927524\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9621480666585177\n",
      "\t average f1 :  0.8660177894601665\n",
      "\t f1 :  [0.79882193 0.9837183  0.79084619 0.87142281 0.88527971]\n",
      "\t confusion matrix :  [[ 1763   304    34    47   102]\n",
      " [  164 40843    72    37    48]\n",
      " [   42   189   743    11    22]\n",
      " [  101   409     3  2162    15]\n",
      " [   94   129    20    15  1717]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.022346439649953836\n",
      "Training metrics:\n",
      "\t accuracy :  0.9932304785894207\n",
      "\t average f1 :  0.9762764772913748\n",
      "\t f1 :  [0.96275876 0.9973784  0.96253982 0.98289213 0.97581328]\n",
      "\t confusion matrix :  [[  9604    218     45     49     92]\n",
      " [   156 168918     63     96     37]\n",
      " [    49    111   4381     14     30]\n",
      " [    42    127      6  10916     24]\n",
      " [    92     80     23     22   8069]]\n",
      "Validating..\n",
      "Validation loss:  0.1654867154393287\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9644094039033533\n",
      "\t average f1 :  0.8704577408669414\n",
      "\t f1 :  [0.80424028 0.9852479  0.79807177 0.88249952 0.88222923]\n",
      "\t confusion matrix :  [[ 1707   325    47    87    84]\n",
      " [   89 40907    52    79    37]\n",
      " [   35   194   745    17    16]\n",
      " [   67   305     2  2302    14]\n",
      " [   97   144    14    42  1678]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.021875539139764717\n",
      "Training metrics:\n",
      "\t accuracy :  0.9930336901763224\n",
      "\t average f1 :  0.9768245996374368\n",
      "\t f1 :  [0.96835665 0.99699792 0.96340127 0.98009658 0.97527057]\n",
      "\t confusion matrix :  [[  9655    195     25     52     77]\n",
      " [   135 168874     92    113     73]\n",
      " [    28    135   4396      9     18]\n",
      " [    50    169      4  10858     23]\n",
      " [    69    105     23     21   8065]]\n",
      "Validating..\n",
      "Validation loss:  0.18694194029133845\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9619850873976287\n",
      "\t average f1 :  0.8690229325528918\n",
      "\t f1 :  [0.80845004 0.98260713 0.80916031 0.8582028  0.88669439]\n",
      "\t confusion matrix :  [[ 1703   367    39    41   100]\n",
      " [   96 40987    32    18    31]\n",
      " [   30   216   742     3    16]\n",
      " [   62   526     0  2082    20]\n",
      " [   72   165    14    18  1706]]\n",
      "********************************************************************************\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  0.018018929573111422\n",
      "Training metrics:\n",
      "\t accuracy :  0.9944948441435768\n",
      "\t average f1 :  0.9811267415885435\n",
      "\t f1 :  [0.97178369 0.99767954 0.96791914 0.98730645 0.98094489]\n",
      "\t confusion matrix :  [[  9695    189     35     33     53]\n",
      " [   126 168970     50     82     52]\n",
      " [    34    115   4405      6     25]\n",
      " [    37     91      6  10967     13]\n",
      " [    56     81     21     14   8108]]\n",
      "Validating..\n",
      "Validation loss:  0.15921731484437865\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9643075418652977\n",
      "\t average f1 :  0.8723047080575304\n",
      "\t f1 :  [0.81602236 0.98461836 0.78314491 0.88418623 0.89355169]\n",
      "\t confusion matrix :  [[ 1752   298    38    67    95]\n",
      " [  133 40776   108   100    47]\n",
      " [   29   180   762    16    20]\n",
      " [   58   300     9  2298    25]\n",
      " [   72   108    22    27  1746]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 2,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h52XPGEg7JOu"
   },
   "source": [
    "Please re-run with `window_size=1`. How does the final performance change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TvikQAmC2aW1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 1, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.18347081637727825\n",
      "Training metrics:\n",
      "\t accuracy :  0.9450369962216625\n",
      "\t average f1 :  0.803391330625591\n",
      "\t f1 :  [0.74593309 0.97725813 0.69874584 0.82484091 0.77017868]\n",
      "\t confusion matrix :  [[  6901   1880    183    442    604]\n",
      " [   557 167332    129    592    676]\n",
      " [   256   1123   2730    261    210]\n",
      " [   308   1638     73   8879    212]\n",
      " [   471   1193    119    245   6250]]\n",
      "Validating..\n",
      "Validation loss:  0.15121307668353742\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9551807032555107\n",
      "\t average f1 :  0.8376290930492166\n",
      "\t f1 :  [0.76551724 0.98232697 0.77825619 0.82436676 0.83767831]\n",
      "\t confusion matrix :  [[ 1665   331    40    48   166]\n",
      " [  154 40826    54    35    95]\n",
      " [   39   190   723    24    31]\n",
      " [  127   490     8  1969    96]\n",
      " [  115   120    26    11  1703]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.04133487315918847\n",
      "Training metrics:\n",
      "\t accuracy :  0.9884239215994962\n",
      "\t average f1 :  0.9528292474869169\n",
      "\t f1 :  [0.92787662 0.99662435 0.91597106 0.9803463  0.9433279 ]\n",
      "\t confusion matrix :  [[  9205    323    115     66    299]\n",
      " [   192 168877     82     75     54]\n",
      " [   134    223   4115     36     80]\n",
      " [    66     84     18  10899     45]\n",
      " [   236    111     67     47   7815]]\n",
      "Validating..\n",
      "Validation loss:  0.14613554652411645\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9600700810821823\n",
      "\t average f1 :  0.8572007301706206\n",
      "\t f1 :  [0.79674061 0.98341769 0.7773238  0.85140073 0.87712082]\n",
      "\t confusion matrix :  [[ 1760   297    59    41    93]\n",
      " [  149 40802    97    57    59]\n",
      " [   34   173   761    24    15]\n",
      " [  123   419     9  2097    42]\n",
      " [  102   125    25    17  1706]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.029723747055719535\n",
      "Training metrics:\n",
      "\t accuracy :  0.9911494411209067\n",
      "\t average f1 :  0.968065553066312\n",
      "\t f1 :  [0.94859555 0.99677384 0.95110132 0.980836   0.96302105]\n",
      "\t confusion matrix :  [[  9439    275     61     45    192]\n",
      " [   194 168850     77     98     52]\n",
      " [    64    145   4318     21     37]\n",
      " [    45    152      9  10876     27]\n",
      " [   147    100     30     28   7982]]\n",
      "Validating..\n",
      "Validation loss:  0.16593709281490496\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9585014056961252\n",
      "\t average f1 :  0.8575511819396203\n",
      "\t f1 :  [0.7791411  0.98196842 0.81151548 0.85602961 0.85910129]\n",
      "\t confusion matrix :  [[ 1651   366    30    64   139]\n",
      " [  147 40762    47   127    81]\n",
      " [   33   181   747    26    20]\n",
      " [   79   381     1  2197    32]\n",
      " [   78   167     9    29  1692]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.027546517082938093\n",
      "Training metrics:\n",
      "\t accuracy :  0.9915282588161209\n",
      "\t average f1 :  0.9705679338380166\n",
      "\t f1 :  [0.95774506 0.99665003 0.9556976  0.97906893 0.96367805]\n",
      "\t confusion matrix :  [[  9531    232     47     39    163]\n",
      " [   152 168837     79    134     75]\n",
      " [    49    149   4336     12     39]\n",
      " [    42    181      4  10852     27]\n",
      " [   117    133     23     25   7986]]\n",
      "Validating..\n",
      "Validation loss:  0.16066795911804851\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9605182740496272\n",
      "\t average f1 :  0.8605340613552984\n",
      "\t f1 :  [0.80531592 0.98309365 0.79463807 0.84914842 0.87047425]\n",
      "\t confusion matrix :  [[ 1727   332    43    44   104]\n",
      " [  105 40879    55    70    55]\n",
      " [   30   189   741    22    25]\n",
      " [   73   463     4  2094    56]\n",
      " [  104   137    15    12  1707]]\n",
      "********************************************************************************\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  0.023891667118741747\n",
      "Training metrics:\n",
      "\t accuracy :  0.9923842884130982\n",
      "\t average f1 :  0.9724131028568561\n",
      "\t f1 :  [0.95975077 0.99715448 0.95525014 0.98330777 0.96660236]\n",
      "\t confusion matrix :  [[  9550    209     51     39    163]\n",
      " [   138 168907     72     81     65]\n",
      " [    46    152   4344     12     31]\n",
      " [    44    138     12  10898     21]\n",
      " [   111    109     31     23   8017]]\n",
      "Validating..\n",
      "Validation loss:  0.18055415582299852\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9596015157071263\n",
      "\t average f1 :  0.8624216685629978\n",
      "\t f1 :  [0.80154515 0.9820073  0.81619256 0.83818075 0.87418258]\n",
      "\t confusion matrix :  [[ 1660   401    29    72    88]\n",
      " [   65 40879    37   143    40]\n",
      " [   27   182   746    38    14]\n",
      " [   36   471     1  2147    35]\n",
      " [  104   159     8    33  1671]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 1,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGgA0zExVtg9"
   },
   "source": [
    "### Question **(1 point)**\n",
    "\n",
    "If everything works as expected, you should see the loss decrease and the accuracy increase for both training and validation. The final accuracy can be pretty high; you should probably debug if it's below 92%. However, **is accuracy a good metric for this problem? Why?**. Hint: look at the F1 scores for different tags and the confusion matrix.\n",
    "\n",
    "**TODO: Please fill in your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy might not be the best metric for Named Entity Recognition (NER) tasks. Here's why:\n",
    "\n",
    "1. **Imbalanced Classes**: NER datasets often have imbalanced classes, where some tags (like 'O' for non-entity tokens) are much more frequent than others (like specific entity tags). Accuracy can be misleading in such cases because a model that always predicts the majority class can still achieve high accuracy.\n",
    "\n",
    "2. **F1 Score**: The F1 score, which is the harmonic mean of precision and recall, is a better metric for NER tasks. It considers both false positives and false negatives, providing a more balanced evaluation of the model's performance on each tag.\n",
    "\n",
    "3. **Confusion Matrix**: The confusion matrix can help identify specific types of errors the model is making, such as confusing one entity type with another. This detailed insight is not captured by accuracy alone.\n",
    "\n",
    "In summary, while accuracy gives a general idea of performance, the F1 score and confusion matrix provide more detailed and meaningful insights for NER tasks.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
