{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6YTnpgbFdMI",
    "outputId": "e8a79660-2c2b-4a57-cf57-1cf069eb7c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  9 21:02:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti     Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P3               9W /  45W |      6MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2848      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.2.0 torchtext==0.17.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install -U scikit-learn\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c62StNb2NvKk",
    "outputId": "03be5bda-0ee1-40d8-bdd2-38b3b9f6eb81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.2.0+rocm5.7\n",
      "Torchtext Version: 0.17.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Torchtext Version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EjRM4cCFRh-d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn1bIPjAN-9V"
   },
   "source": [
    "## Feedforward Neural Network (FFNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJOKIneRTrTH"
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVt1a6nzWsiF"
   },
   "source": [
    "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooQEJEafWCcd"
   },
   "source": [
    "First, we write a dataloader for loading the dataset into mini-batches used for training the model. See [torch.utils.data](https://pytorch.org/docs/stable/data.html) for how dataloaders work in PyTorch. In short, we typically need to do two things:\n",
    "\n",
    "1. Define a [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets) by subclassing [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and overriding 3 methods: `__init__`, `__getitem__`, and `__len__`.\n",
    "1. Create a [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) by calling its constructor. We have to specify the dataset and a few hyperparameters such as batch size.\n",
    "\n",
    "Most of the work has been done by us. As a simple exercise, try to understand the code and implement `__len__`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WnNfOBUYJvVW"
   },
   "outputs": [],
   "source": [
    "# A sentence is a list of (word, tag) tuples.\n",
    "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
    "Sentence = List[Tuple[str, str]]\n",
    "\n",
    "\n",
    "def read_data_file(\n",
    "    datapath: str,\n",
    ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Read and preprocess input data from the file `datapath`.\n",
    "    Example:\n",
    "    ```\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
    "    ```\n",
    "    Return values:\n",
    "        `sentences`: a list of sentences, including words and NER tags\n",
    "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
    "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
    "    \"\"\"\n",
    "    sentences: List[Sentence] = []\n",
    "    word_cnt: Dict[str, int] = Counter()\n",
    "    tag_cnt: Dict[str, int] = Counter()\n",
    "\n",
    "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
    "        if \"DOCSTART\" in sentence_txt:\n",
    "            # Ignore dummy sentences at the begining of each document.\n",
    "            continue\n",
    "        # Read a new sentence\n",
    "        sentences.append([])\n",
    "        for token in sentence_txt.split(\"\\n\"):\n",
    "            w, _, _, t = token.split()\n",
    "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
    "            w = re.sub(\"\\d\", \"0\", w)\n",
    "            word_cnt[w] += 1\n",
    "            tag_cnt[t] += 1\n",
    "            sentences[-1].append((w, t))\n",
    "\n",
    "    return sentences, word_cnt, tag_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFVnIjA8KEe0"
   },
   "source": [
    "## Implement the `__len__` function below **(1 point)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9RGv1K0pP1bR"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each data example is a word, its NER tag (the target), and a fixed window centered around it (the input).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        datapath: str,\n",
    "        window_size: int,\n",
    "        words_vocab: Optional[Vocab] = None,\n",
    "        tags_vocab: Optional[Vocab] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset by reading from datapath.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "        START = \"<START>\"\n",
    "        END = \"<END>\"\n",
    "        UNKNOWN = \"<UNKNOWN>\"\n",
    "\n",
    "        print(\"Loading data from %s\" % datapath)\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
    "\n",
    "        # Extract windows\n",
    "        for sent in sentences:\n",
    "            words = [START for _ in range(window_size)]\n",
    "            tags = [None for _ in range(window_size)]\n",
    "            for w, t in sent:\n",
    "                words.append(w)\n",
    "                tags.append(t)\n",
    "            words.extend([END for _ in range(window_size)])\n",
    "            tags.extend([None for _ in range(window_size)])\n",
    "\n",
    "            for i, t in enumerate(tags[window_size:-window_size], start=window_size):\n",
    "                self.examples.append(\n",
    "                    {\n",
    "                        \"word\": words[i],\n",
    "                        \"tag\": t,\n",
    "                        \"context\": words[i - window_size : i + window_size + 1],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print(\"%d examples loaded.\" % len(self.examples))\n",
    "\n",
    "        # set vocabs\n",
    "        if words_vocab is None:\n",
    "            words_vocab = vocab(word_cnt, specials=[START, END, UNKNOWN]) # automatically create a vocabulary from words in dataset\n",
    "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
    "        self.words_vocab = words_vocab\n",
    "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
    "        self.start_idx = self.words_vocab[START]\n",
    "        self.end_idx = self.words_vocab[END]\n",
    "\n",
    "        if tags_vocab is None:\n",
    "            tags_vocab = vocab(tag_cnt, specials=[]) # automatically create tags vocabulary from tags in dataset\n",
    "        self.tags_vocab = tags_vocab\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the idx'th example in the dataset.\n",
    "        Convert words and the tag to indexes.\n",
    "        \"\"\"\n",
    "        example = self.examples[idx]\n",
    "        word = example[\"word\"]\n",
    "        tag = example[\"tag\"]\n",
    "        context = example[\"context\"]\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"word_idx\": self.words_vocab[word],\n",
    "            \"tag\": tag,\n",
    "            \"tag_idx\": self.tags_vocab[tag],\n",
    "            \"context\": context,\n",
    "            \"context_idxs\": torch.tensor(\n",
    "                [self.words_vocab[w] for w in context]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of examples in the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        return len(self.examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F-aHcN5yJyoa"
   },
   "outputs": [],
   "source": [
    "def create_fixed_window_dataloaders(\n",
    "    batch_size: int, window_size: int, shuffle: bool = True\n",
    ") -> Tuple[DataLoader, DataLoader, Dict[str, Vocab]]:\n",
    "    \"\"\"\n",
    "    Create the dataloaders for training and validaiton.\n",
    "    \"\"\"\n",
    "    ds_train = FixedWindowDataset(\"eng.train\", window_size)\n",
    "    # Re-use the vocabulary of the training data\n",
    "    ds_val = FixedWindowDataset(\"eng.val\", window_size, words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
    "    loader_train = DataLoader(\n",
    "        ds_train, batch_size, shuffle, drop_last=True, pin_memory=True\n",
    "    )\n",
    "    loader_val = DataLoader(ds_val, batch_size, pin_memory=True)\n",
    "    return loader_train, loader_val, ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODfPyQjPSmCv"
   },
   "source": [
    "Let's test our dataloader. Try to understand the output, as it will save your time later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "Zmt9c9svgzy8",
    "outputId": "cce6bfd1-8029-4220-84be-0c855807c25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "Iterating on the training data..\n",
      "{'word': ['EU', 'rejects', 'German'], 'word_idx': tensor([3, 4, 5]), 'tag': ['ORG', 'O', 'MISC'], 'tag_idx': tensor([0, 1, 2]), 'context': [('<START>', '<START>', 'EU'), ('<START>', 'EU', 'rejects'), ('EU', 'rejects', 'German'), ('rejects', 'German', 'call'), ('German', 'call', 'to')], 'context_idxs': tensor([[0, 0, 3, 4, 5],\n",
      "        [0, 3, 4, 5, 6],\n",
      "        [3, 4, 5, 6, 7]])}\n",
      "5\n",
      "torch.Size([3, 5])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def check_fixed_window_dataloader() -> None:\n",
    "    loader_train, _, _ = create_fixed_window_dataloaders(\n",
    "        batch_size=3, window_size=2, shuffle=False\n",
    "    )\n",
    "    print(\"Iterating on the training data..\")\n",
    "    for i, data_batch in enumerate(loader_train):\n",
    "        if i == 0:\n",
    "            print(data_batch)\n",
    "            print(len(data_batch[\"context\"]))\n",
    "            print(data_batch[\"context_idxs\"].shape)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "check_fixed_window_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR_aJ-FincuN"
   },
   "source": [
    "### Implement the Model **(4 points)**\n",
    "\n",
    "Next, let's implement feedforward neural networks following the description of Problem 1 in Assignment #3.\n",
    "\n",
    "Models in PyTorch are subclasses of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You have to override `__init__` for initializing the model and `forward` for calculating the forward pass. Checkout this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html#) if you are not sure how torch.nn.Module works.\n",
    "\n",
    "PyTorch provides a wide array of [neural network layers](https://pytorch.org/docs/stable/nn.html) as building blocks for your model. Here are some of them that may be relevant:\n",
    "\n",
    "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid) or [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
    "\n",
    "Note a difference with Problem 3 of Assignment #2 is that we do not apply softmax when calculatinng $\\hat{y}^{(t)}$. Instead, we leave what softmax does to the loss function [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy). For details, please see its difference with [F.nll_loss](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Hx0oMgVffSD1"
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward Neural Networks for NER\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, words_vocab: Vocab, tags_vocab: Vocab, window_size: int, d_emb: int, d_hidden: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a two-layer feedforward neural network with sigmoid activation.\n",
    "        Parameters:\n",
    "            `words_vocab`: vocabulary of words\n",
    "            `tags_vocab`: vocabulary of tags\n",
    "            `window_size`: size of the context window (w in Problem 3 of Assignment #2)\n",
    "            `d_emb`: dimension of word embeddings (D in Problem 3 of Assignment #2)\n",
    "            `d_hidden`: dimension of the hidden layer (H in Problem 3 of Assignment #2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Create the word embeddings (nn.Embedding),\n",
    "        self.words_vocab = words_vocab\n",
    "        self.tags_vocab = tags_vocab\n",
    "        self.window_size = window_size\n",
    "        self.d_emb = d_emb\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
    "        self.hidden_layer = nn.Linear((2 * window_size + 1) * d_emb, d_hidden)\n",
    "        self.output_layer = nn.Linear(d_hidden, len(tags_vocab))\n",
    "\n",
    "    def forward(self, context_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given the word indexes in a context window, predict the logits of the NER tag.\n",
    "        Parameters:\n",
    "            `context_idxs`: a batch_size x (2 * window_size + 1) tensor\n",
    "                          context_idxs[i] contains word indexes in the window of the i'th data example.\n",
    "        Return values:\n",
    "            `logits`: a batch_size x 5 tensor (\\hat{y}^{(t)} in Problem 3 of Assignment #2, without softmax)\n",
    "                    logits[i][j] is the output score (before softmax) of the i'th example for tag j.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the two-layer FFNN with sigmoid hidden layer.\n",
    "        #       Do not apply softmax, since we will use F.cross_entropy as the loss function.\n",
    "        \n",
    "                                                    # context_idxs: batch_size x (2 * window_size + 1) tensor\n",
    "        context_emb = self.embedding(context_idxs)  # batch_size x (2 * window_size + 1) x d_emb tensor\n",
    "        batch_size, _, _ = context_emb.size()\n",
    "        context_flat = context_emb.view(batch_size, -1)\n",
    "        hidden_output = torch.sigmoid(self.hidden_layer(context_flat))\n",
    "        logits = self.output_layer(hidden_output)   # batch_size x 5 tensor\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyZCvfOMR7YP"
   },
   "source": [
    "Optionally, let's do a simple sanity check of your implementation. In `check_ffnn`, we load a batch of data examples and pass it through the FFNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WLMGYSZ7KxzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Some helper code\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Use GPU when it is available; use CPU otherwise.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H7jikP_1fZP-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=448, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "Input tensor shape: torch.Size([3, 7])\n",
      "Output tensor shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "def check_ffnn() -> None:\n",
    "  # Hyperparameters\n",
    "  batch_size = 3\n",
    "  d_emb = 64\n",
    "  d_hidden = 128\n",
    "  window_size = 3\n",
    "  # Create the dataloaders and the model\n",
    "  loader_train, _, ds_train = create_fixed_window_dataloaders(batch_size, window_size)\n",
    "  model = FFNN(ds_train.words_vocab, ds_train.tags_vocab, window_size, d_emb, d_hidden)\n",
    "  device = get_device()\n",
    "  model.to(device)\n",
    "  print(model)\n",
    "  # Get the first batch\n",
    "  data_batch = next(iter(loader_train))\n",
    "  # Move data to GPU\n",
    "  context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "  tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "  # Calculate the model\n",
    "  print(\"Input tensor shape:\", context_idxs.size())\n",
    "  logits = model(context_idxs)\n",
    "  print(\"Output tensor shape:\", logits.size())\n",
    "\n",
    "check_ffnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PkRIOgeXIng0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n"
     ]
    }
   ],
   "source": [
    "ds_train = FixedWindowDataset(\"eng.train\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0angAnEno9v"
   },
   "source": [
    "### Training and Validation **(4 points)**\n",
    "\n",
    "Having implemented the model, the next step is to implement functions for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vWcKwiIMjekY"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate various evaluation metrics such as accuracy and F1 score\n",
    "    Parameters:\n",
    "        `ground_truth`: the list of ground truth NER tags\n",
    "        `predictions`: the list of predicted NER tags\n",
    "    \"\"\"\n",
    "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
    "        \"average f1\": np.mean(f1_scores),\n",
    "        \"f1\": f1_scores,\n",
    "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_ffnn(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    silent: bool = False,  # whether to print the training loss\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train the FFNN model.\n",
    "    Return values:\n",
    "        1. the average training loss\n",
    "        2. training metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    report_interval = 100\n",
    "\n",
    "    for i, data_batch in enumerate(loader):\n",
    "        context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "        tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
    "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
    "        # 3. Perform the backward pass to calculate the gradient.\n",
    "        # 4. Use the optimizer to update model parameters.\n",
    "        # Caveat: You may need to call optimizer.zero_grad(). Figure out what it does!\n",
    "        \n",
    "        logits = model(context_idxs)\n",
    "        loss = F.cross_entropy(logits, tag_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ground_truth.extend(tag_idx.tolist())\n",
    "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "        if not silent and i > 0 and i % report_interval == 0:\n",
    "            print(\n",
    "                \"\\t[%06d/%06d] Loss: %f\"\n",
    "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
    "            )\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def validate_ffnn(\n",
    "    model: nn.Module, loader: DataLoader, device: torch.device\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate the FFNN model.\n",
    "    Return values:\n",
    "        1. the average validation loss\n",
    "        2. validation metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data_batch in loader:\n",
    "            context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "            tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "            # TODO: Similar to what you did in train_ffnn, but only step 1 and 2.\n",
    "\n",
    "            logits = model(context_idxs)\n",
    "            loss = F.cross_entropy(logits, tag_idx)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            ground_truth.extend(tag_idx.tolist())\n",
    "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def train_val_loop_ffnn(hyperparams: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Train and validate the FFNN model for a number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Hyperparameters:\", hyperparams)\n",
    "    # Create the dataloaders\n",
    "    loader_train, loader_val, ds_train = create_fixed_window_dataloaders(\n",
    "        hyperparams[\"batch_size\"], hyperparams[\"window_size\"]\n",
    "    )\n",
    "    # Create the model\n",
    "    model = FFNN(\n",
    "        ds_train.words_vocab,\n",
    "        ds_train.tags_vocab,\n",
    "        hyperparams[\"window_size\"],\n",
    "        hyperparams[\"d_emb\"],\n",
    "        hyperparams[\"d_hidden\"],\n",
    "    )\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # Create the optimizer\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
    "    )\n",
    "\n",
    "    # Train and validate\n",
    "    for i in range(hyperparams[\"num_epochs\"]):\n",
    "        print(\"*\" * 80 )\n",
    "        print(f\"Epoch #{i+1}\")\n",
    "\n",
    "        print(\"Training..\")\n",
    "        loss_train, metrics_train = train_ffnn(\n",
    "            model, loader_train, optimizer, device, silent=True\n",
    "        )\n",
    "        print(\"Training loss: \", loss_train)\n",
    "        print(\"Training metrics:\")\n",
    "        for k, v in metrics_train.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "        print(\"Validating..\")\n",
    "        loss_val, metrics_val = validate_ffnn(model, loader_val, device)\n",
    "        print(\"Validation loss: \", loss_val)\n",
    "        print(\"Validation metrics:\")\n",
    "        for k, v in metrics_val.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "    print(\"************ Training Done! ************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbuC7sYHX3Wl"
   },
   "source": [
    "We are ready to run experiments! Let's train the model for 5 epochs, with `window_size=2`. After each epoch, we perform validation and print the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5ZTOFj9NXl65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 2, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=320, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.18752444131060572\n",
      "Training metrics:\n",
      "\t accuracy :  0.9448648063602015\n",
      "\t average f1 :  0.8018547587051851\n",
      "\t f1 :  [0.74054702 0.97656275 0.66527402 0.82443958 0.80245042]\n",
      "\t confusion matrix :  [[  6796   2018    231    467    499]\n",
      " [   623 167356    391    644    263]\n",
      " [   256   1209   2707    153    258]\n",
      " [   311   1699     77   8845    175]\n",
      " [   357   1186    149    241   6353]]\n",
      "Validating..\n",
      "Validation loss:  0.1346748655778356\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9615165220225727\n",
      "\t average f1 :  0.8560587113127409\n",
      "\t f1 :  [0.79015355 0.98459047 0.76304348 0.88419083 0.85831523]\n",
      "\t confusion matrix :  [[ 1621   310    47   104   168]\n",
      " [   98 40733    70   128   135]\n",
      " [   41   203   702    23    38]\n",
      " [   35   242     1  2363    49]\n",
      " [   58    89    13    37  1778]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.03617366921785346\n",
      "Training metrics:\n",
      "\t accuracy :  0.9897670025188917\n",
      "\t average f1 :  0.9597849437289776\n",
      "\t f1 :  [0.93987644 0.99666805 0.92591769 0.97961021 0.95685233]\n",
      "\t confusion matrix :  [[  9356    309     88     69    188]\n",
      " [   205 168856     85     80     57]\n",
      " [    94    215   4162     38     74]\n",
      " [    72     91     19  10882     41]\n",
      " [   172     87     53     43   7928]]\n",
      "Validating..\n",
      "Validation loss:  0.1545787629620463\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9619239701747953\n",
      "\t average f1 :  0.8631587956349683\n",
      "\t f1 :  [0.79815175 0.9833129  0.77818952 0.87416174 0.88197808]\n",
      "\t confusion matrix :  [[ 1641   380    50    67   112]\n",
      " [   94 40895    66    54    55]\n",
      " [   32   203   735    17    20]\n",
      " [   36   399     8  2216    31]\n",
      " [   59   137    23    26  1730]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.022619895731457874\n",
      "Training metrics:\n",
      "\t accuracy :  0.9933879093198993\n",
      "\t average f1 :  0.9765201335071328\n",
      "\t f1 :  [0.96445224 0.99741094 0.96051908 0.98504774 0.97517066]\n",
      "\t confusion matrix :  [[  9618    210     47     44     93]\n",
      " [   154 168928     65     82     58]\n",
      " [    50    119   4367     16     30]\n",
      " [    35    101     10  10936     25]\n",
      " [    76     88     22     19   8071]]\n",
      "Validating..\n",
      "Validation loss:  0.15390142620587235\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9619647149900176\n",
      "\t average f1 :  0.8668474097334261\n",
      "\t f1 :  [0.80170863 0.9836844  0.78126608 0.87789276 0.88968518]\n",
      "\t confusion matrix :  [[ 1783   249    41    89    88]\n",
      " [  233 40606   104   153    68]\n",
      " [   44   167   759    21    16]\n",
      " [   67   260     8  2333    22]\n",
      " [   71   113    24    29  1738]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.02197485941319119\n",
      "Training metrics:\n",
      "\t accuracy :  0.9930927267002518\n",
      "\t average f1 :  0.9770475501143651\n",
      "\t f1 :  [0.96689738 0.99704787 0.96465423 0.9807987  0.97583957]\n",
      "\t confusion matrix :  [[  9639    208     28     49     82]\n",
      " [   151 168869     76    122     66]\n",
      " [    33    126   4394      8     25]\n",
      " [    51    147      5  10880     23]\n",
      " [    58    104     21     21   8078]]\n",
      "Validating..\n",
      "Validation loss:  0.1592973822262138\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9634926455608523\n",
      "\t average f1 :  0.870662922619444\n",
      "\t f1 :  [0.7981588  0.98440137 0.79893617 0.87835455 0.89346372]\n",
      "\t confusion matrix :  [[ 1734   312    40    86    78]\n",
      " [  153 40831    67    50    63]\n",
      " [   48   179   751    15    14]\n",
      " [   80   346     2  2242    20]\n",
      " [   80   124    13    22  1736]]\n",
      "********************************************************************************\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  0.01792848785290546\n",
      "Training metrics:\n",
      "\t accuracy :  0.9942832965994962\n",
      "\t average f1 :  0.9803742188487279\n",
      "\t f1 :  [0.97318928 0.99761135 0.96707638 0.98526297 0.97873112]\n",
      "\t confusion matrix :  [[  9728    162     22     40     60]\n",
      " [   128 168938     72     83     61]\n",
      " [    23    117   4406      7     27]\n",
      " [    40    105     11  10931     19]\n",
      " [    61     81     21     22   8099]]\n",
      "Validating..\n",
      "Validation loss:  0.1706652142772024\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9620258322128509\n",
      "\t average f1 :  0.866670001460933\n",
      "\t f1 :  [0.80521817 0.98361446 0.78831369 0.86645213 0.88975155]\n",
      "\t confusion matrix :  [[ 1790   288    47    52    73]\n",
      " [  181 40790    96    47    50]\n",
      " [   34   179   769    12    13]\n",
      " [  109   386     7  2154    34]\n",
      " [   82   132    25    17  1719]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 2,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h52XPGEg7JOu"
   },
   "source": [
    "Please re-run with `window_size=1`. How does the final performance change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TvikQAmC2aW1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 1, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.18278633777793468\n",
      "Training metrics:\n",
      "\t accuracy :  0.9454354927581864\n",
      "\t average f1 :  0.8045951413786071\n",
      "\t f1 :  [0.74138211 0.97710196 0.68659716 0.82891048 0.788984  ]\n",
      "\t confusion matrix :  [[  6850   1902    247    451    562]\n",
      " [   550 167359    253    576    547]\n",
      " [   351   1108   2733    176    214]\n",
      " [   323   1718     34   8871    156]\n",
      " [   393   1190    112    228   6360]]\n",
      "Validating..\n",
      "Validation loss:  0.14408108143349332\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9553640549240109\n",
      "\t average f1 :  0.8398596878413714\n",
      "\t f1 :  [0.76190476 0.98072959 0.76614216 0.84455225 0.84596967]\n",
      "\t confusion matrix :  [[ 1592   445    49    62   102]\n",
      " [   88 40918    49    65    44]\n",
      " [   36   218   706    22    25]\n",
      " [   61   508     9  2089    23]\n",
      " [  152   191    23    19  1590]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.0400335152354326\n",
      "Training metrics:\n",
      "\t accuracy :  0.988724023929471\n",
      "\t average f1 :  0.954629853178743\n",
      "\t f1 :  [0.92782674 0.99670337 0.92249695 0.98069394 0.94542826]\n",
      "\t confusion matrix :  [[  9211    311    122     68    297]\n",
      " [   217 168857     79     66     62]\n",
      " [   126    197   4160     36     68]\n",
      " [    61     84     19  10896     51]\n",
      " [   231    101     52     44   7848]]\n",
      "Validating..\n",
      "Validation loss:  0.15400552217518756\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9574420405003463\n",
      "\t average f1 :  0.8452388216570801\n",
      "\t f1 :  [0.78141136 0.98215275 0.74253201 0.85005005 0.87004794]\n",
      "\t confusion matrix :  [[ 1589   351    96    64   150]\n",
      " [   71 40778   179    80    56]\n",
      " [   19   172   783    16    17]\n",
      " [   69   441    16  2123    41]\n",
      " [   69   132    28    22  1724]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.028392140128384685\n",
      "Training metrics:\n",
      "\t accuracy :  0.9917299669395466\n",
      "\t average f1 :  0.9700390683293814\n",
      "\t f1 :  [0.95141721 0.99707181 0.95589203 0.98131725 0.96449704]\n",
      "\t confusion matrix :  [[  9449    247     72     46    192]\n",
      " [   176 168892     70    105     44]\n",
      " [    49    120   4356     20     39]\n",
      " [    44    139      5  10899     25]\n",
      " [   139     91     27     31   7987]]\n",
      "Validating..\n",
      "Validation loss:  0.1711354307772126\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9594792812614594\n",
      "\t average f1 :  0.8621469885257058\n",
      "\t f1 :  [0.80518845 0.98112756 0.80814577 0.84679894 0.86947423]\n",
      "\t confusion matrix :  [[ 1645   419    37    55    94]\n",
      " [   64 40966    47    39    48]\n",
      " [   18   204   754    17    14]\n",
      " [   43   543     4  2070    30]\n",
      " [   66   212    17    18  1662]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.027654340501255367\n",
      "Training metrics:\n",
      "\t accuracy :  0.9913659083753149\n",
      "\t average f1 :  0.969616386426393\n",
      "\t f1 :  [0.95451118 0.99672673 0.95505988 0.97804229 0.96374184]\n",
      "\t confusion matrix :  [[  9495    238     57     60    163]\n",
      " [   159 168848     80    117     71]\n",
      " [    41    145   4346     14     37]\n",
      " [    58    177      9  10846     23]\n",
      " [   129    122     26     29   7974]]\n",
      "Validating..\n",
      "Validation loss:  0.16071613630386614\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9592959295929593\n",
      "\t average f1 :  0.8587690361867854\n",
      "\t f1 :  [0.7823516  0.9826133  0.80582524 0.85507246 0.86798257]\n",
      "\t confusion matrix :  [[ 1720   330    31    58   111]\n",
      " [  177 40804    53    61    69]\n",
      " [   40   190   747    16    14]\n",
      " [  106   420     1  2124    39]\n",
      " [  104   144    15    19  1693]]\n",
      "********************************************************************************\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  0.02305715389183772\n",
      "Training metrics:\n",
      "\t accuracy :  0.9927139090050378\n",
      "\t average f1 :  0.9746065025274306\n",
      "\t f1 :  [0.95972683 0.9971751  0.9627425  0.98358882 0.96979926]\n",
      "\t confusion matrix :  [[  9556    217     44     44    152]\n",
      " [   153 168908     65    102     58]\n",
      " [    41    130   4367      9     28]\n",
      " [    41    129      5  10908     19]\n",
      " [   110    103     16     15   8044]]\n",
      "Validating..\n",
      "Validation loss:  0.16227149437812236\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9592959295929593\n",
      "\t average f1 :  0.8520852004686412\n",
      "\t f1 :  [0.78786419 0.98309051 0.75510204 0.86152056 0.8728487 ]\n",
      "\t confusion matrix :  [[ 1636   318    74   118   104]\n",
      " [  110 40755   150    90    59]\n",
      " [   33   163   777    17    17]\n",
      " [   46   378     6  2221    39]\n",
      " [   78   134    44    20  1699]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 1,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGgA0zExVtg9"
   },
   "source": [
    "### Question **(1 point)**\n",
    "\n",
    "If everything works as expected, you should see the loss decrease and the accuracy increase for both training and validation. The final accuracy can be pretty high; you should probably debug if it's below 92%. However, **is accuracy a good metric for this problem? Why?**. Hint: look at the F1 scores for different tags and the confusion matrix.\n",
    "\n",
    "**TODO: Please fill in your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy might not be the best metric for Named Entity Recognition (NER) tasks. Here's why:\n",
    "\n",
    "1. **Imbalanced Classes**: NER datasets often have imbalanced classes, where some tags (like 'O' for non-entity tokens) are much more frequent than others (like specific entity tags). Accuracy can be misleading in such cases because a model that always predicts the majority class can still achieve high accuracy.\n",
    "\n",
    "2. **F1 Score**: The F1 score, which is the harmonic mean of precision and recall, is a better metric for NER tasks. It considers both false positives and false negatives, providing a more balanced evaluation of the model's performance on each tag.\n",
    "\n",
    "3. **Confusion Matrix**: The confusion matrix can help identify specific types of errors the model is making, such as confusing one entity type with another. This detailed insight is not captured by accuracy alone.\n",
    "\n",
    "In summary, while accuracy gives a general idea of performance, the F1 score and confusion matrix provide more detailed and meaningful insights for NER tasks.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
