{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GAWytObKh4K"
   },
   "source": [
    "## Use GPUs in Colab\n",
    "\n",
    "GPUs are not strictly necessary for this assignment, but they will accerlerate your experiments and may be helpful for your final project. To enable the free GPU provided by Colab, just go to **Edit > Notebook settings** and select **GPU** as **Hardware accelerator**. To test your GPU, run the `nvidia-smi` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6YTnpgbFdMI",
    "outputId": "e8a79660-2c2b-4a57-cf57-1cf069eb7c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 23:27:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti     Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P0              21W /  80W |      6MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2926      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.2.0 torchtext==0.17.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install -U scikit-learn\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbgSdkFMMa3E"
   },
   "source": [
    "You should see a table summarizing the current status of the GPU, including the model, power consumption, GPU memory usage, etc. However, Colab does not guarantee which model you will get. We tested the assignment successfully on Tesla T4 but had some problems on other GPUs. **Please make sure you get a Tesla T4** by checking the output of `nvidia-smi`. We empirically observe that you are more likely to get a T4 **with your Princeton account**.\n",
    "\n",
    "If you are still unable to get a T4. You have 3 options:\n",
    "\n",
    "1. Reload the page until you get one.\n",
    "2. Proceed with other GPUs. They should work as long as Colab does not crash when executing code.\n",
    "3. Disable the GPU and use the CPU for this assignment, which is totally fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26HdExpsKTOg"
   },
   "source": [
    "## Learning PyTorch\n",
    "\n",
    "This assignment assumes basic familiarity with [PyTorch](https://pytorch.org/), which is a much needed skill for the upcoming Assignment #4, for the final project, and even beyond this course. If you aren't familiar with PyTorch, don't worry, it's a great time to start now! Here are some resources to help you get started. You may also find plenty of them online.\n",
    "\n",
    "- We will cover PyTorch in the precept on 3/18. Watch the video on Canvas if you are not able to make it.\n",
    "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c62StNb2NvKk",
    "outputId": "03be5bda-0ee1-40d8-bdd2-38b3b9f6eb81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.2.0+rocm5.7\n",
      "Torchtext Version: 0.17.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Torchtext Version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhV4CYivRbt4"
   },
   "source": [
    "Let's import all the packages at once:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EjRM4cCFRh-d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn1bIPjAN-9V"
   },
   "source": [
    "## Feedforward Neural Network (FFNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJOKIneRTrTH"
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVt1a6nzWsiF"
   },
   "source": [
    "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooQEJEafWCcd"
   },
   "source": [
    "First, we write a dataloader for loading the dataset into mini-batches used for training the model. See [torch.utils.data](https://pytorch.org/docs/stable/data.html) for how dataloaders work in PyTorch. In short, we typically need to do two things:\n",
    "\n",
    "1. Define a [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets) by subclassing [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and overriding 3 methods: `__init__`, `__getitem__`, and `__len__`.\n",
    "1. Create a [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) by calling its constructor. We have to specify the dataset and a few hyperparameters such as batch size.\n",
    "\n",
    "Most of the work has been done by us. As a simple exercise, try to understand the code and implement `__len__`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WnNfOBUYJvVW"
   },
   "outputs": [],
   "source": [
    "# A sentence is a list of (word, tag) tuples.\n",
    "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
    "Sentence = List[Tuple[str, str]]\n",
    "\n",
    "\n",
    "def read_data_file(\n",
    "    datapath: str,\n",
    ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Read and preprocess input data from the file `datapath`.\n",
    "    Example:\n",
    "    ```\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
    "    ```\n",
    "    Return values:\n",
    "        `sentences`: a list of sentences, including words and NER tags\n",
    "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
    "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
    "    \"\"\"\n",
    "    sentences: List[Sentence] = []\n",
    "    word_cnt: Dict[str, int] = Counter()\n",
    "    tag_cnt: Dict[str, int] = Counter()\n",
    "\n",
    "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
    "        if \"DOCSTART\" in sentence_txt:\n",
    "            # Ignore dummy sentences at the begining of each document.\n",
    "            continue\n",
    "        # Read a new sentence\n",
    "        sentences.append([])\n",
    "        for token in sentence_txt.split(\"\\n\"):\n",
    "            w, _, _, t = token.split()\n",
    "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
    "            w = re.sub(\"\\d\", \"0\", w)\n",
    "            word_cnt[w] += 1\n",
    "            tag_cnt[t] += 1\n",
    "            sentences[-1].append((w, t))\n",
    "\n",
    "    return sentences, word_cnt, tag_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFVnIjA8KEe0"
   },
   "source": [
    "## Implement the `__len__` function below **(1 point)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9RGv1K0pP1bR"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each data example is a word, its NER tag (the target), and a fixed window centered around it (the input).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        datapath: str,\n",
    "        window_size: int,\n",
    "        words_vocab: Optional[Vocab] = None,\n",
    "        tags_vocab: Optional[Vocab] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset by reading from datapath.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "        START = \"<START>\"\n",
    "        END = \"<END>\"\n",
    "        UNKNOWN = \"<UNKNOWN>\"\n",
    "\n",
    "        print(\"Loading data from %s\" % datapath)\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
    "\n",
    "        # Extract windows\n",
    "        for sent in sentences:\n",
    "            words = [START for _ in range(window_size)]\n",
    "            tags = [None for _ in range(window_size)]\n",
    "            for w, t in sent:\n",
    "                words.append(w)\n",
    "                tags.append(t)\n",
    "            words.extend([END for _ in range(window_size)])\n",
    "            tags.extend([None for _ in range(window_size)])\n",
    "\n",
    "            for i, t in enumerate(tags[window_size:-window_size], start=window_size):\n",
    "                self.examples.append(\n",
    "                    {\n",
    "                        \"word\": words[i],\n",
    "                        \"tag\": t,\n",
    "                        \"context\": words[i - window_size : i + window_size + 1],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print(\"%d examples loaded.\" % len(self.examples))\n",
    "\n",
    "        # set vocabs\n",
    "        if words_vocab is None:\n",
    "            words_vocab = vocab(word_cnt, specials=[START, END, UNKNOWN]) # automatically create a vocabulary from words in dataset\n",
    "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
    "        self.words_vocab = words_vocab\n",
    "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
    "        self.start_idx = self.words_vocab[START]\n",
    "        self.end_idx = self.words_vocab[END]\n",
    "\n",
    "        if tags_vocab is None:\n",
    "            tags_vocab = vocab(tag_cnt, specials=[]) # automatically create tags vocabulary from tags in dataset\n",
    "        self.tags_vocab = tags_vocab\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the idx'th example in the dataset.\n",
    "        Convert words and the tag to indexes.\n",
    "        \"\"\"\n",
    "        example = self.examples[idx]\n",
    "        word = example[\"word\"]\n",
    "        tag = example[\"tag\"]\n",
    "        context = example[\"context\"]\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"word_idx\": self.words_vocab[word],\n",
    "            \"tag\": tag,\n",
    "            \"tag_idx\": self.tags_vocab[tag],\n",
    "            \"context\": context,\n",
    "            \"context_idxs\": torch.tensor(\n",
    "                [self.words_vocab[w] for w in context]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of examples in the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        return len(self.examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F-aHcN5yJyoa"
   },
   "outputs": [],
   "source": [
    "def create_fixed_window_dataloaders(\n",
    "    batch_size: int, window_size: int, shuffle: bool = True\n",
    ") -> Tuple[DataLoader, DataLoader, Dict[str, Vocab]]:\n",
    "    \"\"\"\n",
    "    Create the dataloaders for training and validaiton.\n",
    "    \"\"\"\n",
    "    ds_train = FixedWindowDataset(\"eng.train\", window_size)\n",
    "    # Re-use the vocabulary of the training data\n",
    "    ds_val = FixedWindowDataset(\"eng.val\", window_size, words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
    "    loader_train = DataLoader(\n",
    "        ds_train, batch_size, shuffle, drop_last=True, pin_memory=True\n",
    "    )\n",
    "    loader_val = DataLoader(ds_val, batch_size, pin_memory=True)\n",
    "    return loader_train, loader_val, ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODfPyQjPSmCv"
   },
   "source": [
    "Let's test our dataloader. Try to understand the output, as it will save your time later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "Zmt9c9svgzy8",
    "outputId": "cce6bfd1-8029-4220-84be-0c855807c25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "Iterating on the training data..\n",
      "{'word': ['EU', 'rejects', 'German'], 'word_idx': tensor([3, 4, 5]), 'tag': ['ORG', 'O', 'MISC'], 'tag_idx': tensor([0, 1, 2]), 'context': [('<START>', '<START>', 'EU'), ('<START>', 'EU', 'rejects'), ('EU', 'rejects', 'German'), ('rejects', 'German', 'call'), ('German', 'call', 'to')], 'context_idxs': tensor([[0, 0, 3, 4, 5],\n",
      "        [0, 3, 4, 5, 6],\n",
      "        [3, 4, 5, 6, 7]])}\n",
      "5\n",
      "torch.Size([3, 5])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def check_fixed_window_dataloader() -> None:\n",
    "    loader_train, _, _ = create_fixed_window_dataloaders(\n",
    "        batch_size=3, window_size=2, shuffle=False\n",
    "    )\n",
    "    print(\"Iterating on the training data..\")\n",
    "    for i, data_batch in enumerate(loader_train):\n",
    "        if i == 0:\n",
    "            print(data_batch)\n",
    "            print(len(data_batch[\"context\"]))\n",
    "            print(data_batch[\"context_idxs\"].shape)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "check_fixed_window_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR_aJ-FincuN"
   },
   "source": [
    "### Implement the Model **(4 points)**\n",
    "\n",
    "Next, let's implement feedforward neural networks following the description of Problem 1 in Assignment #3.\n",
    "\n",
    "Models in PyTorch are subclasses of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You have to override `__init__` for initializing the model and `forward` for calculating the forward pass. Checkout this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html#) if you are not sure how torch.nn.Module works.\n",
    "\n",
    "PyTorch provides a wide array of [neural network layers](https://pytorch.org/docs/stable/nn.html) as building blocks for your model. Here are some of them that may be relevant:\n",
    "\n",
    "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid) or [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
    "\n",
    "Note a difference with Problem 3 of Assignment #2 is that we do not apply softmax when calculatinng $\\hat{y}^{(t)}$. Instead, we leave what softmax does to the loss function [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy). For details, please see its difference with [F.nll_loss](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Hx0oMgVffSD1"
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward Neural Networks for NER\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, words_vocab: Vocab, tags_vocab: Vocab, window_size: int, d_emb: int, d_hidden: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a two-layer feedforward neural network with sigmoid activation.\n",
    "        Parameters:\n",
    "            `words_vocab`: vocabulary of words\n",
    "            `tags_vocab`: vocabulary of tags\n",
    "            `window_size`: size of the context window (w in Problem 3 of Assignment #2)\n",
    "            `d_emb`: dimension of word embeddings (D in Problem 3 of Assignment #2)\n",
    "            `d_hidden`: dimension of the hidden layer (H in Problem 3 of Assignment #2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Create the word embeddings (nn.Embedding),\n",
    "        self.words_vocab = words_vocab\n",
    "        self.tags_vocab = tags_vocab\n",
    "        self.window_size = window_size\n",
    "        self.d_emb = d_emb\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
    "        self.hidden_layer = nn.Linear((2 * window_size + 1) * d_emb, d_hidden)\n",
    "        self.output_layer = nn.Linear(d_hidden, len(tags_vocab))\n",
    "\n",
    "    def forward(self, context_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given the word indexes in a context window, predict the logits of the NER tag.\n",
    "        Parameters:\n",
    "            `context_idxs`: a batch_size x (2 * window_size + 1) tensor\n",
    "                          context_idxs[i] contains word indexes in the window of the i'th data example.\n",
    "        Return values:\n",
    "            `logits`: a batch_size x 5 tensor (\\hat{y}^{(t)} in Problem 3 of Assignment #2, without softmax)\n",
    "                    logits[i][j] is the output score (before softmax) of the i'th example for tag j.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the two-layer FFNN with sigmoid hidden layer.\n",
    "        #       Do not apply softmax, since we will use F.cross_entropy as the loss function.\n",
    "        \n",
    "        context_emb = self.embedding(context_idxs)\n",
    "        batch_size, _, _ = context_emb.size()\n",
    "        context_flat = context_emb.view(batch_size, -1)\n",
    "        hidden_output = torch.sigmoid(self.hidden_layer(context_flat))\n",
    "        logits = self.output_layer(hidden_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyZCvfOMR7YP"
   },
   "source": [
    "Optionally, let's do a simple sanity check of your implementation. In `check_ffnn`, we load a batch of data examples and pass it through the FFNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WLMGYSZ7KxzP"
   },
   "outputs": [],
   "source": [
    "# Some helper code\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Use GPU when it is available; use CPU otherwise.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H7jikP_1fZP-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=448, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "Input tensor shape: torch.Size([3, 7])\n",
      "Output tensor shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "def check_ffnn() -> None:\n",
    "  # Hyperparameters\n",
    "  batch_size = 3\n",
    "  d_emb = 64\n",
    "  d_hidden = 128\n",
    "  window_size = 3\n",
    "  # Create the dataloaders and the model\n",
    "  loader_train, _, ds_train = create_fixed_window_dataloaders(batch_size, window_size)\n",
    "  model = FFNN(ds_train.words_vocab, ds_train.tags_vocab, window_size, d_emb, d_hidden)\n",
    "  device = get_device()\n",
    "  model.to(device)\n",
    "  print(model)\n",
    "  # Get the first batch\n",
    "  data_batch = next(iter(loader_train))\n",
    "  # Move data to GPU\n",
    "  context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "  tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "  # Calculate the model\n",
    "  print(\"Input tensor shape:\", context_idxs.size())\n",
    "  logits = model(context_idxs)\n",
    "  print(\"Output tensor shape:\", logits.size())\n",
    "\n",
    "check_ffnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PkRIOgeXIng0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "203621 examples loaded.\n"
     ]
    }
   ],
   "source": [
    "ds_train = FixedWindowDataset(\"eng.train\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0angAnEno9v"
   },
   "source": [
    "### Training and Validation **(4 points)**\n",
    "\n",
    "Having implemented the model, the next step is to implement functions for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vWcKwiIMjekY"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate various evaluation metrics such as accuracy and F1 score\n",
    "    Parameters:\n",
    "        `ground_truth`: the list of ground truth NER tags\n",
    "        `predictions`: the list of predicted NER tags\n",
    "    \"\"\"\n",
    "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
    "        \"average f1\": np.mean(f1_scores),\n",
    "        \"f1\": f1_scores,\n",
    "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_ffnn(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    silent: bool = False,  # whether to print the training loss\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train the FFNN model.\n",
    "    Return values:\n",
    "        1. the average training loss\n",
    "        2. training metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    report_interval = 100\n",
    "\n",
    "    for i, data_batch in enumerate(loader):\n",
    "        context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "        tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
    "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
    "        # 3. Perform the backward pass to calculate the gradient.\n",
    "        # 4. Use the optimizer to update model parameters.\n",
    "        # Caveat: You may need to call optimizer.zero_grad(). Figure out what it does!\n",
    "        \n",
    "        logits = model(context_idxs)\n",
    "        loss = F.cross_entropy(logits, tag_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ground_truth.extend(tag_idx.tolist())\n",
    "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "        if not silent and i > 0 and i % report_interval == 0:\n",
    "            print(\n",
    "                \"\\t[%06d/%06d] Loss: %f\"\n",
    "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
    "            )\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def validate_ffnn(\n",
    "    model: nn.Module, loader: DataLoader, device: torch.device\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate the FFNN model.\n",
    "    Return values:\n",
    "        1. the average validation loss\n",
    "        2. validation metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data_batch in loader:\n",
    "            context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
    "            tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
    "\n",
    "            # TODO: Similar to what you did in train_ffnn, but only step 1 and 2.\n",
    "\n",
    "            logits = model(context_idxs)\n",
    "            loss = F.cross_entropy(logits, tag_idx)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            ground_truth.extend(tag_idx.tolist())\n",
    "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def train_val_loop_ffnn(hyperparams: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Train and validate the FFNN model for a number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Hyperparameters:\", hyperparams)\n",
    "    # Create the dataloaders\n",
    "    loader_train, loader_val, ds_train = create_fixed_window_dataloaders(\n",
    "        hyperparams[\"batch_size\"], hyperparams[\"window_size\"]\n",
    "    )\n",
    "    # Create the model\n",
    "    model = FFNN(\n",
    "        ds_train.words_vocab,\n",
    "        ds_train.tags_vocab,\n",
    "        hyperparams[\"window_size\"],\n",
    "        hyperparams[\"d_emb\"],\n",
    "        hyperparams[\"d_hidden\"],\n",
    "    )\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # Create the optimizer\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
    "    )\n",
    "\n",
    "    # Train and validate\n",
    "    for i in range(hyperparams[\"num_epochs\"]):\n",
    "        print(\"*\" * 80 )\n",
    "        print(\"Epoch #%d\" % i)\n",
    "\n",
    "        print(\"Training..\")\n",
    "        loss_train, metrics_train = train_ffnn(\n",
    "            model, loader_train, optimizer, device, silent=True\n",
    "        )\n",
    "        print(\"Training loss: \", loss_train)\n",
    "        print(\"Training metrics:\")\n",
    "        for k, v in metrics_train.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "        print(\"Validating..\")\n",
    "        loss_val, metrics_val = validate_ffnn(model, loader_val, device)\n",
    "        print(\"Validation loss: \", loss_val)\n",
    "        print(\"Validation metrics:\")\n",
    "        for k, v in metrics_val.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "    print(\"************ Training Done! ************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbuC7sYHX3Wl"
   },
   "source": [
    "We are ready to run experiments! Let's train the model for 5 epochs, with `window_size=2`. After each epoch, we perform validation and print the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5ZTOFj9NXl65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 2, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=320, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #0\n",
      "Training..\n",
      "Training loss:  0.18386425382413102\n",
      "Training metrics:\n",
      "\t accuracy :  0.9461390113350125\n",
      "\t average f1 :  0.8112116230794539\n",
      "\t f1 :  [0.74354194 0.97666904 0.70634418 0.83001316 0.7994898 ]\n",
      "\t confusion matrix :  [[  6980   1887    184    470    493]\n",
      " [   857 167446    148    558    273]\n",
      " [   252   1197   2789    117    228]\n",
      " [   290   1791     53   8833    136]\n",
      " [   382   1289    140    203   6268]]\n",
      "Validating..\n",
      "Validation loss:  0.13677215373172658\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9615572668377949\n",
      "\t average f1 :  0.8606795117028356\n",
      "\t f1 :  [0.79287202 0.98368932 0.77870216 0.86694594 0.88118812]\n",
      "\t confusion matrix :  [[ 1713   354    43    51    89]\n",
      " [  115 40920    31    57    41]\n",
      " [   46   218   702    16    25]\n",
      " [  111   382     7  2173    17]\n",
      " [   86   159    13    26  1691]]\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.03519766694943535\n",
      "Training metrics:\n",
      "\t accuracy :  0.990008068324937\n",
      "\t average f1 :  0.960614308756983\n",
      "\t f1 :  [0.94073888 0.99680979 0.92958685 0.98098107 0.95495495]\n",
      "\t confusion matrix :  [[  9358    295     96     73    185]\n",
      " [   191 168884     72     77     61]\n",
      " [    94    199   4185     29     78]\n",
      " [    66     85      9  10909     40]\n",
      " [   179    101     57     44   7897]]\n",
      "Validating..\n",
      "Validation loss:  0.14658287596648734\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9628407285172962\n",
      "\t average f1 :  0.8653116648387831\n",
      "\t f1 :  [0.80712303 0.98421313 0.76884168 0.88153515 0.88484533]\n",
      "\t confusion matrix :  [[ 1745   326    58    43    78]\n",
      " [  112 40804   113    85    50]\n",
      " [   31   186   760    12    18]\n",
      " [   97   311     7  2251    24]\n",
      " [   89   126    32    26  1702]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.022046388573803814\n",
      "Training metrics:\n",
      "\t accuracy :  0.9933239530856424\n",
      "\t average f1 :  0.9768103174353371\n",
      "\t f1 :  [0.96459733 0.99731045 0.96204203 0.98387968 0.97622209]\n",
      "\t confusion matrix :  [[  9618    219     37     44     90]\n",
      " [   152 168904     66    100     60]\n",
      " [    49    125   4372      9     26]\n",
      " [    39    114      9  10925     22]\n",
      " [    76     75     24     21   8088]]\n",
      "Validating..\n",
      "Validation loss:  0.1613864153802448\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9625758872183514\n",
      "\t average f1 :  0.8672579552396747\n",
      "\t f1 :  [0.8046595  0.98425957 0.79315708 0.87632926 0.87788437]\n",
      "\t confusion matrix :  [[ 1796   279    45    47    83]\n",
      " [  177 40770    86    77    54]\n",
      " [   33   181   765    13    15]\n",
      " [  112   313     3  2225    37]\n",
      " [   96   137    23    26  1693]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.021080462804523952\n",
      "Training metrics:\n",
      "\t accuracy :  0.9933583910579346\n",
      "\t average f1 :  0.9779190197414488\n",
      "\t f1 :  [0.96776458 0.99708634 0.96394812 0.98345878 0.97733728]\n",
      "\t confusion matrix :  [[  9652    221     34     37     65]\n",
      " [   167 168881     67     90     63]\n",
      " [    30    120   4385     18     32]\n",
      " [    33    153      5  10910     14]\n",
      " [    56    106     22     17   8086]]\n",
      "Validating..\n",
      "Validation loss:  0.1628762256562671\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9621480666585177\n",
      "\t average f1 :  0.8592090166001516\n",
      "\t f1 :  [0.80408542 0.98464886 0.74310595 0.88049584 0.88370902]\n",
      "\t confusion matrix :  [[ 1732   284    67    73    94]\n",
      " [  140 40730   158    80    56]\n",
      " [   35   167   768    18    19]\n",
      " [   86   280    16  2273    35]\n",
      " [   65   105    51    29  1725]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.017705019486315022\n",
      "Training metrics:\n",
      "\t accuracy :  0.994366931675063\n",
      "\t average f1 :  0.9802939122002329\n",
      "\t f1 :  [0.97151454 0.99772098 0.96620584 0.98581401 0.98021419]\n",
      "\t confusion matrix :  [[  9686    167     35     47     67]\n",
      " [   117 168985     67     74     45]\n",
      " [    39    117   4403      7     14]\n",
      " [    43     96      9  10945     20]\n",
      " [    53     89     20     19   8100]]\n",
      "Validating..\n",
      "Validation loss:  0.16266728557426782\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9636556248217414\n",
      "\t average f1 :  0.8738159198738824\n",
      "\t f1 :  [0.81371429 0.98383851 0.80430108 0.87930352 0.88792221]\n",
      "\t confusion matrix :  [[ 1780   319    23    44    84]\n",
      " [  156 40817    67    61    63]\n",
      " [   36   182   748    19    22]\n",
      " [   72   364     3  2222    29]\n",
      " [   81   129    12    18  1735]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 2,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h52XPGEg7JOu"
   },
   "source": [
    "Please re-run with `window_size=1`. How does the final performance change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TvikQAmC2aW1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 1, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "203621 examples loaded.\n",
      "Loading data from eng.val\n",
      "49086 examples loaded.\n",
      "FFNN(\n",
      "  (words_vocab): Vocab()\n",
      "  (tags_vocab): Vocab()\n",
      "  (embedding): Embedding(20103, 64)\n",
      "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n",
      "********************************************************************************\n",
      "Epoch #0\n",
      "Training..\n",
      "Training loss:  0.18492110131275594\n",
      "Training metrics:\n",
      "\t accuracy :  0.9444417112720404\n",
      "\t average f1 :  0.8024645918902966\n",
      "\t f1 :  [0.72819352 0.97676441 0.69535759 0.82514458 0.78686285]\n",
      "\t confusion matrix :  [[  6758   2007    223    427    594]\n",
      " [   662 167330    238    586    473]\n",
      " [   342   1060   2771    168    240]\n",
      " [   311   1789     47   8775    180]\n",
      " [   479   1146    110    211   6337]]\n",
      "Validating..\n",
      "Validation loss:  0.1452066220032672\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9570549647557348\n",
      "\t average f1 :  0.8461707021211549\n",
      "\t f1 :  [0.7810236  0.98224382 0.78099399 0.841354   0.8452381 ]\n",
      "\t confusion matrix :  [[ 1671   323    36    50   170]\n",
      " [  136 40825    54    57    92]\n",
      " [   35   202   715    25    30]\n",
      " [   82   478     6  2063    61]\n",
      " [  105   134    13    19  1704]]\n",
      "********************************************************************************\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  0.04095791239450951\n",
      "Training metrics:\n",
      "\t accuracy :  0.9883452062342569\n",
      "\t average f1 :  0.9529713804891415\n",
      "\t f1 :  [0.92581932 0.99663027 0.92016388 0.97925012 0.9429933 ]\n",
      "\t confusion matrix :  [[  9167    333    123     70    306]\n",
      " [   196 168879     87     78     44]\n",
      " [   120    195   4155     34     86]\n",
      " [    68    101     19  10878     43]\n",
      " [   253    108     57     48   7816]]\n",
      "Validating..\n",
      "Validation loss:  0.15041720266162883\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9611905635007946\n",
      "\t average f1 :  0.8619959693254755\n",
      "\t f1 :  [0.78965438 0.98405357 0.80661157 0.86074453 0.86891579]\n",
      "\t confusion matrix :  [[ 1725   326    28    67   104]\n",
      " [  136 40852    35    84    57]\n",
      " [   40   188   732    27    20]\n",
      " [  100   362     3  2185    40]\n",
      " [  118   136    10    24  1687]]\n",
      "********************************************************************************\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  0.02821893232900377\n",
      "Training metrics:\n",
      "\t accuracy :  0.9917496457808564\n",
      "\t average f1 :  0.9696412345536679\n",
      "\t f1 :  [0.95097793 0.99714864 0.95417079 0.98227574 0.96363307]\n",
      "\t confusion matrix :  [[  9457    240     65     42    201]\n",
      " [   154 168910     71     97     51]\n",
      " [    64    134   4341     13     36]\n",
      " [    48    137      9  10890     23]\n",
      " [   161     82     25     24   7989]]\n",
      "Validating..\n",
      "Validation loss:  0.16223187953194915\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9592959295929593\n",
      "\t average f1 :  0.8590022429377256\n",
      "\t f1 :  [0.80113636 0.98167055 0.78873239 0.84873618 0.87473573]\n",
      "\t confusion matrix :  [[ 1692   347    39    67   105]\n",
      " [  123 40864    56    91    30]\n",
      " [   34   202   728    32    11]\n",
      " [   43   489     1  2149     8]\n",
      " [   82   188    15    35  1655]]\n",
      "********************************************************************************\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  0.027583696395604488\n",
      "Training metrics:\n",
      "\t accuracy :  0.9911789593828715\n",
      "\t average f1 :  0.9688744529147174\n",
      "\t f1 :  [0.9548653  0.9966415  0.95290614 0.97706339 0.96289593]\n",
      "\t confusion matrix :  [[  9499    238     49     45    176]\n",
      " [   162 168852     74    129     66]\n",
      " [    56    154   4320     22     31]\n",
      " [    42    192     15  10820     37]\n",
      " [   130    123     26     26   7980]]\n",
      "Validating..\n",
      "Validation loss:  0.1665302069086465\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9601719431202379\n",
      "\t average f1 :  0.8584444219379483\n",
      "\t f1 :  [0.79535917 0.98251345 0.78205805 0.85481305 0.87747839]\n",
      "\t confusion matrix :  [[ 1611   424    40    71   104]\n",
      " [   60 40904    84    68    48]\n",
      " [   25   195   741    26    20]\n",
      " [   37   436     7  2149    61]\n",
      " [   68   141    16    24  1726]]\n",
      "********************************************************************************\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  0.023797650829717628\n",
      "Training metrics:\n",
      "\t accuracy :  0.9925859965365239\n",
      "\t average f1 :  0.9733531712468733\n",
      "\t f1 :  [0.96055078 0.99731395 0.96087052 0.98178868 0.96624192]\n",
      "\t confusion matrix :  [[  9557    192     44     51    166]\n",
      " [   126 168939     65     98     54]\n",
      " [    39    130   4371     12     34]\n",
      " [    45    140      6  10890     26]\n",
      " [   122    105     26     26   8000]]\n",
      "Validating..\n",
      "Validation loss:  0.18444453148792186\n",
      "Validation metrics:\n",
      "\t accuracy :  0.9555881514077333\n",
      "\t average f1 :  0.8432384212169897\n",
      "\t f1 :  [0.76701217 0.98221256 0.80860452 0.79809976 0.86026309]\n",
      "\t confusion matrix :  [[ 1702   352    25    41   130]\n",
      " [  121 40890    36    25    92]\n",
      " [   35   195   733    15    29]\n",
      " [  239   533     0  1848    70]\n",
      " [   91   127    12    12  1733]]\n",
      "************ Training Done! ************\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_ffnn(\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"d_emb\": 64,\n",
    "        \"d_hidden\": 128,\n",
    "        \"window_size\": 1,\n",
    "        \"num_epochs\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"l2\": 1e-6,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGgA0zExVtg9"
   },
   "source": [
    "### Question **(1 point)**\n",
    "\n",
    "If everything works as expected, you should see the loss decrease and the accuracy increase for both training and validation. The final accuracy can be pretty high; you should probably debug if it's below 92%. However, **is accuracy a good metric for this problem? Why?**. Hint: look at the F1 scores for different tags and the confusion matrix.\n",
    "\n",
    "**TODO: Please fill in your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy might not be the best metric for Named Entity Recognition (NER) tasks. Here's why:\n",
    "\n",
    "1. **Imbalanced Classes**: NER datasets often have imbalanced classes, where some tags (like 'O' for non-entity tokens) are much more frequent than others (like specific entity tags). Accuracy can be misleading in such cases because a model that always predicts the majority class can still achieve high accuracy.\n",
    "\n",
    "2. **F1 Score**: The F1 score, which is the harmonic mean of precision and recall, is a better metric for NER tasks. It considers both false positives and false negatives, providing a more balanced evaluation of the model's performance on each tag.\n",
    "\n",
    "3. **Confusion Matrix**: The confusion matrix can help identify specific types of errors the model is making, such as confusing one entity type with another. This detailed insight is not captured by accuracy alone.\n",
    "\n",
    "In summary, while accuracy gives a general idea of performance, the F1 score and confusion matrix provide more detailed and meaningful insights for NER tasks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
